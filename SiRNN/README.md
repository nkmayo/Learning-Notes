# SiRNN
SiRNNs (a portmanteau of Siren and RNN, though still pronouced 'siren') are a little experiment that I wanted to try out. Recurrent Neural Networks (RNNs)have the ability to 'retain' information via a recurrent hidden layer that passes information from the previous step to itself in its next iteration. The problem with RNNs is that they suffer from the vanishing gradient problem in which, for deep RNN networks, weights near the input of the network are unable to be significantly adjusted by gradient descent during backpropagation due to the gradients shrinking between layers when along 'flat' regions of the activation function. Sinusoidal Representation Networks, aka Sirens (https://vsitzmann.github.io/siren/), are essentially fully connected or dense layers with sinusiodal activations that attempt to paramaterize the representation of signal continuously. One benefit of using sinusoidal activations is that their derivatives (also sinusoidal) do not oscillate rather than vanish in the limit of large positive and negative values. This, to me, suggests that Siren networks are less susceptible to the vanishing gradient problem. By integrating the two concepts, I hope to build a simple machine learning 'unit' capable of memory, but not (or less) susceptible to the vanishing gradient problem.